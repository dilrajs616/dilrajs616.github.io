<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Documentation</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <h1 id="project-report">Project Report</h1>
<p>Our goal is to make a model that can categorize websites based on their scraped data. The first problem is that we had non-labelled data. To train our model, we needed a labelled dataset. So we tried clustering websites together based on the keywords that were extracted from their content. But the clusters had too much noise. Our next approach was to use pre-trained models to categorize the text content scraped from websites. We tested two types of models:</p>
<ol>
<li><strong>Fine-tuned models from Hugging Face</strong>: These models are faster, smaller, and they require fewer resources. But the downside is that most of them are trained on some specific categories only, so they will not be able to categorize all the websites accurately.</li>
<li><strong>Open source LLM models from Ollama</strong>: These are popular Large Language Models like Meta Llama, Deepseek V3, etc. These models are very accurate but they are very large in size and require very high resources.<ul>
<li><em>Ollama is a platform where quantized versions of LLM models are published.</em></li>
</ul>
</li>
</ol>
<hr>
<h3 id="we-tested-the-following-models-for-text-classification-from-hugging-face-">We tested the following models for text classification from Hugging Face:</h3>
<ol>
<li><a href="https://huggingface.co/alimazhar-110/website_classification/blob/main/config.json">Ali Mazhar Text Classification</a> (only 15 categories available)</li>
<li><a href="https://huggingface.co/Tiabet/website_classification-finetuned-Tiabet-2/blob/main/config.json">Tiabet 2</a> (Only 5 categories available)</li>
<li><a href="https://huggingface.co/manimaranpa07/mnli_bart_text_classification_1_08th_march_margs/blob/main/config.json">Fine-Tuned Bart Model</a> (Overfitted on training model)</li>
<li><a href="https://huggingface.co/MoritzLaurer/ModernBERT-large-zeroshot-v2.0/blob/main/config.json">Fine-Tuned Zero Shot</a> (Overfitted on training model)</li>
</ol>
<h4 id="there-are-two-types-of-models-here-">There are two types of models here:</h4>
<ol>
<li>The models that directly tell us the category only. The problem is that they are trained on different datasets and they have very limited categories. We cannot fine-tune them based on our requirements.</li>
<li>We define a list of categories, then the model evaluates the text content and tells us what is the most probable category of that content. These are called Zero Shot models For example:<pre><code class="lang-python">sequence_to_classify = <span class="hljs-string">"one day I will see the world"</span>
candidate_labels = [<span class="hljs-string">'travel'</span>, <span class="hljs-string">'cooking'</span>, <span class="hljs-string">'dancing'</span>]
classifier(sequence_to_classify, candidate_labels)
<span class="hljs-meta"># Output:</span>
<span class="hljs-meta"># {<span class="hljs-string">'labels'</span>: [<span class="hljs-string">'travel'</span>, <span class="hljs-string">'dancing'</span>, <span class="hljs-string">'cooking'</span>],</span>
<span class="hljs-meta">#  <span class="hljs-string">'scores'</span>: [0.9938651323318481, 0.0032737774308770895, 0.002861034357920289],</span>
<span class="hljs-meta">#  <span class="hljs-string">'sequence'</span>: <span class="hljs-string">'one day I will see the world'</span>}</span>
</code></pre>
</li>
</ol>
<p>At the end we decided to use <a href="https://huggingface.co/facebook/bart-large-mnli">FaceBook Zero Shot Model</a>.</p>
<hr>
<p>The problem that we faced in this model is that it needs only cleaned text. Most of the websites have a huge paragraph about cookies or other non-relevant information in the scraped data. That makes the results less accurate. To overcome this, we used two methods. </p>
<ol>
<li>We used <a href="https://huggingface.co/facebook/bart-large-cnn">Facebook Bart Large CNN</a> model. It is a text summarizer model. We first summarize the text and then give it to the classifier model. But it significantly increased the processing time and power.</li>
<li>Second solution was to extraxt keywords from the scraped data. This solution is more time efficient.</li>
</ol>
<p><strong>We tested 1000 websites with the following categories. This was just our first iteration to check the initial accuracy.</strong></p>
<p>[&#39;Adult And Dating&#39;, &#39;Advertisements&#39;, &#39;Agriculture&#39;, &#39;Alcohol And Tobacco&#39;, &#39;Articles&#39;, &#39;Astrology&#39;, &#39;Automobiles And Transportation&#39;, &#39;Beauty&#39;, &#39;Biography&#39;, &#39;Blogs And Forums&#39;, &#39;Business&#39;, &#39;Cloudflare&#39;, &#39;Construction&#39;, &#39;Drugs&#39;, &#39;Ecommerce And Shopping&#39;, &#39;Education&#39;, &#39;Entertainment&#39;, &#39;Environment&#39;, &#39;Fashion&#39;, &#39;Finance&#39;, &#39;Food And Beverages&#39;, &#39;Gambling&#39;, &#39;Gaming&#39;, &#39;Government&#39;, &#39;Hacking&#39;, &#39;Health&#39;, &#39;Healthcare&#39;, &#39;Hobbies And Interests&#39;, &#39;Home And Garden&#39;, &#39;Intolerance And Hate&#39;, &#39;Jobs Search&#39;, &#39;Life&#39;, &#39;Logistics&#39;, &#39;Medicine&#39;, &#39;News&#39;, &#39;Non Governmental Organization&#39;, &#39;Parked Domain&#39;, &#39;Personal And Portfolio Website&#39;, &#39;Pets And Animals&#39;, &#39;Politics&#39;, &#39;Pornography&#39;, &#39;Real Estate And Property&#39;, &#39;Religious&#39;, &#39;Research&#39;, &#39;Restaurants And Dining&#39;, &#39;Search Engines&#39;, &#39;Security And Defense&#39;, &#39;Services and Repair&#39;, &#39;Social Media Networking&#39;, &#39;Society And Culture&#39;, &#39;Sports&#39;, &#39;Stock Market&#39;, &#39;Technology&#39;, &#39;Tourism&#39;, &#39;Unknown&#39;]</p>
<p><strong>After the first iteration, we manually categorised those 1000 websites and found out that the accuracy of the model was 67.5 percent. We also found following categories that were not present in our category list in our first iteration.</strong> </p>
<p>[&#39;Adult Entertainment&#39;, &#39;Architecture&#39;, &#39;Association&#39;, &#39;Auditons&#39;, &#39;AutoMobile&#39;, &#39;Automation&#39;, &#39;Automobiles&#39;, &#39;Business/Technology&#39;, &#39;Bussiness/Nonprofit&#39;, &#39;Coding&#39;, &#39;Constuction&#39;, &#39;Craked APPS&#39;, &#39;Crypto&#39;, &#39;Cybersecurity&#39;, &#39;Data Security&#39;, &#39;Dating&#39;, &#39;E- Commerce&#39;, &#39;E-Commerce&#39;, &#39;Ecommerce&#39;, &#39;Entertainment/Online Video Streaming&#39;, &#39;Escort Services&#39;, &#39;Food and Beverages&#39;, &#39;Hospitality&#39;, &#39;House Interior&#39;, &#39;INNOVATION/Reseach&#39;, &#39;Insurance&#39;, &#39;Jewellery&#39;, &#39;Legal&#39;, &#39;Lighting/Innovation&#39;, &#39;MARKETING&#39;, &#39;MARKETING/AUTOMATION&#39;, &#39;Manufacor&#39;, &#39;Manufactror&#39;, &#39;Manufactur&#39;, &#39;Manufacture&#39;, &#39;Manufacture and supplier&#39;, &#39;Marketing&#39;, &#39;Media/News&#39;, &#39;Music&#39;, &#39;NGO/Charity&#39;, &#39;News/ Blogs&#39;, &#39;ONLINE TOOLS STORE&#39;, &#39;Online Interview Preparation&#39;, &#39;Online Pdf Convertor&#39;, &#39;Online Store&#39;, &#39;Payments&#39;, &#39;Printing&#39;, &#39;Real Estate&#39;, &#39;Recycleing&#39;, &#39;Redirect To Google&#39;, &#39;Rewards And Coupens&#39;, &#39;SUPPLEMENTS&#39;, &#39;Social Media&#39;, &#39;Startup&#39;, &#39;Stationery Items&#39;, &#39;Techonology&#39;, &#39;Trust &amp; Safety&#39;, &#39;UNCATEGORIZED&#39;, &#39;Visualization/Technology&#39;, &#39;WOOD&#39;, &#39;malicious&#39;, &#39;pharmaceutical&#39;]</p>
<p><strong>With this updated list, we will run the same websites again and test the accuracy.</strong></p>
<p><strong>*Note: These models also need a translate for multilingual websites.</strong></p>
<hr>
<h3 id="ai-models">AI Models</h3>
<p>For improved accuracy, we need at least two predictions, first one from Facebook Zero Shot model and another one from an LLM model. These LLM models are trained for a variety of tasks including answering complex questions. They can be very useful in text categorisation due to these 3 facts:</p>
<ol>
<li>These models can easily ignore non-relevant information with a simple prompt.</li>
<li>Some of these LLMs are trained to be multilingual, that means we don&#39;t need to translate the content.</li>
<li>The accuracy is very high of even the smaller models.</li>
</ol>
<p>We did the research and found out that the largest and most capable LLM is Llama3.1-instruct-405b</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Benchmark</th>
<th>Llama 3.1 405B</th>
<th>Nemotron 4 340B Instruct</th>
<th>GPT-4 (0125)</th>
<th>GPT-4 Omni</th>
<th>Claude 3.5 Sonnet</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>General</strong></td>
<td>MMLU (0-shot, CoT)</td>
<td>88.6</td>
<td>78.7 (non-CoT)</td>
<td>85.4</td>
<td>88.7</td>
<td>88.3</td>
</tr>
<tr>
<td></td>
<td>MMLU PRO (5-shot, CoT)</td>
<td>73.3</td>
<td>62.7</td>
<td>64.8</td>
<td>74.0</td>
<td>77.0</td>
</tr>
<tr>
<td></td>
<td>IFEval</td>
<td>88.6</td>
<td>85.1</td>
<td>84.3</td>
<td>85.6</td>
<td>88.0</td>
</tr>
<tr>
<td><strong>Code</strong></td>
<td>HumanEval (0-shot)</td>
<td>89.0</td>
<td>73.2</td>
<td>86.6</td>
<td>90.2</td>
<td>92.0</td>
</tr>
<tr>
<td></td>
<td>MBPP EvalPlus (base) (0-shot)</td>
<td>88.6</td>
<td>72.8</td>
<td>83.6</td>
<td>87.8</td>
<td>90.5</td>
</tr>
<tr>
<td><strong>Math</strong></td>
<td>GSM8K (8-shot, CoT)</td>
<td>96.8</td>
<td>92.3 (0-shot)</td>
<td>94.2</td>
<td>96.1</td>
<td>96.4 (0-shot)</td>
</tr>
<tr>
<td></td>
<td>MATH (0-shot, CoT)</td>
<td>73.8</td>
<td>41.1</td>
<td>64.5</td>
<td>76.6</td>
<td>71.1</td>
</tr>
<tr>
<td><strong>Reasoning</strong></td>
<td>ARC Challenge (0-shot)</td>
<td>96.9</td>
<td>94.6</td>
<td>96.4</td>
<td>96.7</td>
<td>96.7</td>
</tr>
<tr>
<td></td>
<td>GPOQA (0-shot, CoT)</td>
<td>51.1</td>
<td>-</td>
<td>41.4</td>
<td>53.6</td>
<td>59.4</td>
</tr>
<tr>
<td><strong>Tool use</strong></td>
<td>BFCL</td>
<td>88.5</td>
<td>86.5</td>
<td>88.3</td>
<td>80.5</td>
<td>90.2</td>
</tr>
<tr>
<td></td>
<td>Nexus</td>
<td>58.7</td>
<td>-</td>
<td>50.3</td>
<td>56.1</td>
<td>45.7</td>
</tr>
<tr>
<td><strong>Long context</strong></td>
<td>ZeroSCROLLS/QUALITY</td>
<td>95.2</td>
<td>-</td>
<td>95.2</td>
<td>90.5</td>
<td>90.5</td>
</tr>
<tr>
<td></td>
<td>InfiniteBench/En.MC</td>
<td>83.4</td>
<td>-</td>
<td>72.1</td>
<td>82.5</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>NIH/Multi-needle</td>
<td>98.1</td>
<td>-</td>
<td>100.0</td>
<td>100.0</td>
<td>90.8</td>
</tr>
<tr>
<td><strong>Multilingual</strong></td>
<td>Multilingual MGSM (0-shot)</td>
<td>91.6</td>
<td>-</td>
<td>85.9</td>
<td>90.5</td>
<td>91.6</td>
</tr>
</tbody>
</table>
<p><strong>The open source models available to use locally on our systems are always in quantised form</strong></p>
<h3 id="-explanation-of-quantization-types-"><strong>Explanation of Quantization Types</strong></h3>
<p>Quantization is a technique used to reduce the memory and computational requirements of AI models by representing weights with fewer bits. The different quantization types listed here refer to how the weights are stored and processed. Here’s what they mean:</p>
<hr>
<h3 id="-1-fp16-full-precision-16-bit-floating-point-"><strong>1. <code>fp16</code> (Full-Precision 16-bit Floating Point)</strong></h3>
<ul>
<li><strong>Type:</strong> Floating point  </li>
<li><strong>Description:</strong> Uses <strong>16-bit floating-point (FP16)</strong> numbers to represent weights.  </li>
<li><strong>Pros:</strong> Higher precision, better accuracy.  </li>
<li><strong>Cons:</strong> Requires more memory compared to lower-bit quantized models.  </li>
<li><strong>Use Case:</strong> Best for high-performance inference with minimal accuracy loss.  </li>
</ul>
<hr>
<h3 id="-2-q2_k-q3_k_l-q3_k_m-q3_k_s-2-bit-3-bit-quantization-"><strong>2. <code>q2_K</code>, <code>q3_K_L</code>, <code>q3_K_M</code>, <code>q3_K_S</code> (2-bit &amp; 3-bit Quantization)</strong></h3>
<ul>
<li><strong>Type:</strong> Low-bit integer quantization  </li>
<li><strong>Description:</strong>  <ul>
<li><code>q2_K</code> → <strong>2-bit quantization</strong> (very aggressive compression).  </li>
<li><code>q3_K_L</code> → <strong>3-bit quantization (Large variant)</strong>.  </li>
<li><code>q3_K_M</code> → <strong>3-bit quantization (Medium variant)</strong>.  </li>
<li><code>q3_K_S</code> → <strong>3-bit quantization (Small variant)</strong>.  </li>
</ul>
</li>
<li><strong>Pros:</strong> Very small model size, extremely fast inference.  </li>
<li><strong>Cons:</strong> High accuracy loss compared to higher-bit quantizations.  </li>
<li><strong>Use Case:</strong> Suitable for <strong>edge devices</strong> or applications needing extreme memory efficiency.  </li>
</ul>
<hr>
<h3 id="-3-q4_0-q4_1-q4_k_m-q4_k_s-4-bit-quantization-"><strong>3. <code>q4_0</code>, <code>q4_1</code>, <code>q4_K_M</code>, <code>q4_K_S</code> (4-bit Quantization)</strong></h3>
<ul>
<li><strong>Type:</strong> Moderate quantization  </li>
<li><strong>Description:</strong>  <ul>
<li><code>q4_0</code> → <strong>4-bit quantization (basic method, less accurate)</strong>.  </li>
<li><code>q4_1</code> → <strong>4-bit quantization (slightly better precision than <code>q4_0</code>)</strong>.  </li>
<li><code>q4_K_M</code> → <strong>4-bit quantization (Medium variant, optimized for speed/accuracy balance)</strong>.  </li>
<li><code>q4_K_S</code> → <strong>4-bit quantization (Small variant, highly compressed)</strong>.  </li>
</ul>
</li>
<li><strong>Pros:</strong> Good balance of speed, compression, and accuracy.  </li>
<li><strong>Cons:</strong> Slight performance drop compared to FP16.  </li>
<li><strong>Use Case:</strong> Ideal for <strong>cloud inference</strong> or situations where <strong>model size matters but accuracy is still important</strong>.  </li>
</ul>
<hr>
<h3 id="-4-q5_0-q5_1-q5_k_m-q5_k_s-5-bit-quantization-"><strong>4. <code>q5_0</code>, <code>q5_1</code>, <code>q5_K_M</code>, <code>q5_K_S</code> (5-bit Quantization)</strong></h3>
<ul>
<li><strong>Type:</strong> Higher precision quantization  </li>
<li><strong>Description:</strong>  <ul>
<li><code>q5_0</code> → <strong>5-bit quantization (basic method)</strong>.  </li>
<li><code>q5_1</code> → <strong>5-bit quantization (slightly better accuracy than <code>q5_0</code>)</strong>.  </li>
<li><code>q5_K_M</code> → <strong>5-bit quantization (Medium variant, accuracy-focused)</strong>.  </li>
<li><code>q5_K_S</code> → <strong>5-bit quantization (Small variant, optimized for storage)</strong>.  </li>
</ul>
</li>
<li><strong>Pros:</strong> Higher accuracy than 4-bit quantization while keeping good compression.  </li>
<li><strong>Cons:</strong> Slightly more memory usage than 4-bit.  </li>
<li><strong>Use Case:</strong> Useful when <strong>better accuracy is needed but still with compression benefits</strong>.  </li>
</ul>
<hr>
<h3 id="-5-q6_k-6-bit-quantization-"><strong>5. <code>q6_K</code> (6-bit Quantization)</strong></h3>
<ul>
<li><strong>Type:</strong> High precision quantization  </li>
<li><strong>Description:</strong>  <ul>
<li>Uses <strong>6-bit integer weights</strong>, maintaining a strong balance between compression and accuracy.  </li>
</ul>
</li>
<li><strong>Pros:</strong> Very close to FP16 performance while reducing memory usage significantly.  </li>
<li><strong>Cons:</strong> Slightly larger than lower-bit quantizations but much more efficient than FP16.  </li>
<li><strong>Use Case:</strong> <strong>Best for deployment where accuracy is critical but memory is still a concern</strong>.  </li>
</ul>
<hr>
<h3 id="-6-q8_0-8-bit-quantization-"><strong>6. <code>q8_0</code> (8-bit Quantization)</strong></h3>
<ul>
<li><strong>Type:</strong> Near full-precision quantization  </li>
<li><strong>Description:</strong>  <ul>
<li>Uses <strong>8-bit integers</strong> instead of floating-point numbers.  </li>
</ul>
</li>
<li><strong>Pros:</strong>  <ul>
<li>Minimal accuracy loss compared to FP16.  </li>
<li>Faster inference speed.  </li>
<li>Lower memory footprint than FP16 but higher than lower-bit quantizations.  </li>
</ul>
</li>
<li><strong>Cons:</strong> Requires more memory than 4-bit or 5-bit quantizations.  </li>
<li><strong>Use Case:</strong> Best for <strong>latency-sensitive applications where accuracy still matters</strong>.  </li>
</ul>
<hr>
<h3 id="-summary-of-quantization-types-"><strong>Summary of Quantization Types</strong></h3>
<table>
<thead>
<tr>
<th><strong>Quantization Type</strong></th>
<th><strong>Bit Size</strong></th>
<th><strong>Pros</strong></th>
<th><strong>Cons</strong></th>
<th><strong>Best Use Case</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>fp16</code></strong></td>
<td>16-bit</td>
<td>High precision, best accuracy</td>
<td>High memory usage</td>
<td>Best for performance-critical tasks</td>
</tr>
<tr>
<td><strong><code>q2_K</code></strong></td>
<td>2-bit</td>
<td>Extremely small model size</td>
<td>High accuracy loss</td>
<td>Extreme memory-constrained environments</td>
</tr>
<tr>
<td><strong><code>q3_K_L/M/S</code></strong></td>
<td>3-bit</td>
<td>Very efficient</td>
<td>Significant accuracy loss</td>
<td>Edge devices, ultra-low memory use</td>
</tr>
<tr>
<td><strong><code>q4_0, q4_1, q4_K_M/S</code></strong></td>
<td>4-bit</td>
<td>Good compression-accuracy tradeoff</td>
<td>Some performance drop</td>
<td>Cloud inference, mobile AI</td>
</tr>
<tr>
<td><strong><code>q5_0, q5_1, q5_K_M/S</code></strong></td>
<td>5-bit</td>
<td>Higher accuracy than 4-bit</td>
<td>Slightly larger size</td>
<td>General-purpose AI tasks</td>
</tr>
<tr>
<td><strong><code>q6_K</code></strong></td>
<td>6-bit</td>
<td>Almost FP16 accuracy</td>
<td>Slightly higher memory usage</td>
<td>Balanced accuracy vs efficiency</td>
</tr>
<tr>
<td><strong><code>q8_0</code></strong></td>
<td>8-bit</td>
<td>Near-FP16 performance, fast</td>
<td>Uses more memory than 4-bit</td>
<td>Latency-sensitive applications</td>
</tr>
</tbody>
</table>
<hr>
<p><strong>Q4 is the standard quantisation for all models. We used the q4 quantised version of Llama3 (not 3.1) for testing.</strong></p>
<p>First we downloaded a <a href="https://github.com/dmarcelinobr/Datasets/blob/master/Fortune1000.csv">dataset</a> of top 1000 websites from github. We were able to scrape 363 websites out of them. These websites were of following domains.</p>
<p>[&#39;Energy&#39; &#39;Motor Vehicles &amp; Parts&#39; &#39;Wholesalers&#39; &#39;Technology&#39; &#39;Financials&#39;
 &#39;Retailing&#39; &#39;Food &amp; Drug Stores&#39; &#39;Aerospace &amp; Defense&#39;
 &#39;Household Products&#39; &#39;Health Care&#39; &#39;Food, Beverages &amp; Tobacco&#39;
 &#39;Chemicals&#39; &#39;Industrials&#39; &#39;Materials&#39; &#39;Transportation&#39;
 &#39;Business Services&#39; &#39;Media&#39; &#39;Hotels, Resturants &amp; Leisure&#39;
 &#39;Engineering &amp; Construction&#39; &#39;Apparel&#39; &#39;Telecommunications&#39;]</p>
<p> <strong>We were able to achieve an accuracy of 90 percent. Following were the websites that were categorized incorrectly.</strong></p>
<table>
<thead>
<tr>
<th>Website</th>
<th>Predicted Category</th>
<th>Industry</th>
<th>Sector</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="http://www.dow.com">Dow</a></td>
<td>Business/Industry</td>
<td>Chemicals</td>
<td>Chemicals</td>
</tr>
<tr>
<td><a href="http://www.intel.com">Intel</a></td>
<td>International organization</td>
<td>Technology</td>
<td>Semiconductors and Other Electronic Components</td>
</tr>
<tr>
<td><a href="http://www.chsinc.com">CHS Inc</a></td>
<td>Company/Enterprise</td>
<td>Food, Beverages &amp; Tobacco</td>
<td>Food Production</td>
</tr>
<tr>
<td><a href="http://www.pmi.com">PMI</a></td>
<td>Corporate/E-commerce</td>
<td>Food, Beverages &amp; Tobacco</td>
<td>Tobacco</td>
</tr>
<tr>
<td><a href="http://www.bakerhughes.com">Baker Hughes</a></td>
<td>Corporate/Industrial</td>
<td>Energy</td>
<td>Oil and Gas Equipment, Services</td>
</tr>
<tr>
<td><a href="http://www.northropgrumman.com">Northrop Grumman</a></td>
<td>Business/Industry</td>
<td>Aerospace &amp; Defense</td>
<td>Aerospace and Defense</td>
</tr>
<tr>
<td><a href="http://www.internationalpaper.com">International Paper</a></td>
<td>International organization</td>
<td>Materials</td>
<td>Packaging, Containers</td>
</tr>
<tr>
<td><a href="http://www.lear.com">Lear</a></td>
<td>Company</td>
<td>Motor Vehicles &amp; Parts</td>
<td>Motor Vehicles and Parts</td>
</tr>
<tr>
<td><a href="http://www.edisoninvestor.com">Edison Investor</a></td>
<td>Investment/Corporate Affairs</td>
<td>Energy</td>
<td>Utilities: Gas and Electric</td>
</tr>
<tr>
<td><a href="http://www.praxair.com">Praxair</a></td>
<td>Industrial/Manufacturing</td>
<td>Chemicals</td>
<td>Chemicals</td>
</tr>
<tr>
<td><a href="http://www.libertyinteractive.com">Liberty Interactive</a></td>
<td>E-commerce Investment/Finance</td>
<td>Technology</td>
<td>Internet Services and Retailing</td>
</tr>
<tr>
<td><a href="http://www.darden.com">Darden</a></td>
<td>Education</td>
<td>Hotels, Restaurants &amp; Leisure</td>
<td>Food Services</td>
</tr>
<tr>
<td><a href="http://www.reynoldsamerican.com">Reynolds American</a></td>
<td>Health/Non-Profit, Business</td>
<td>Food, Beverages &amp; Tobacco</td>
<td>Tobacco</td>
</tr>
<tr>
<td><a href="http://www.fmctechnologies.com">FMC Technologies</a></td>
<td>Technology/Telecommunications</td>
<td>Energy</td>
<td>Oil and Gas Equipment, Services</td>
</tr>
<tr>
<td><a href="http://www.cmc.com">CMC</a></td>
<td>Art/Design Education</td>
<td>Materials</td>
<td>Metals</td>
</tr>
<tr>
<td><a href="http://www.ncr.com">NCR</a></td>
<td>E-commerce Financial Services</td>
<td>Technology</td>
<td>Computers, Office Equipment</td>
</tr>
<tr>
<td><a href="http://www.mrcglobal.com">MRC Global</a></td>
<td>Engineering/Manufacturing Business</td>
<td>Energy</td>
<td>Oil and Gas Equipment, Services</td>
</tr>
<tr>
<td><a href="http://www.jbhunt.com">J.B. Hunt</a></td>
<td>E-commerce</td>
<td>Transportation</td>
<td>Trucking, Truck Leasing</td>
</tr>
<tr>
<td><a href="http://www.leidos.com">Leidos</a></td>
<td>Government</td>
<td>Technology</td>
<td>Information Technology Services</td>
</tr>
<tr>
<td><a href="http://www.sonoco.com">Sonoco</a></td>
<td>Business/Economy</td>
<td>Materials</td>
<td>Packaging, Containers</td>
</tr>
<tr>
<td><a href="http://www.aleris.com">Aleris</a></td>
<td>Documentation/Tech Support</td>
<td>Materials</td>
<td>Metals</td>
</tr>
<tr>
<td><a href="http://www.libertymedia.com">Liberty Media</a></td>
<td>Financial/Corporate</td>
<td>Media</td>
<td>Entertainment</td>
</tr>
<tr>
<td><a href="http://www.meadjohnson.com">Mead Johnson</a></td>
<td>Healthcare</td>
<td>Food, Beverages &amp; Tobacco</td>
<td>Food Consumer Products</td>
</tr>
<tr>
<td><a href="http://www.westlake.com">Westlake</a></td>
<td>Corporate News Business/Finance</td>
<td>Chemicals</td>
<td>Chemicals</td>
</tr>
<tr>
<td><a href="http://www.graphicpkg.com">Graphic Packaging</a></td>
<td>Software/Design</td>
<td>Materials</td>
<td>Packaging, Containers</td>
</tr>
<tr>
<td><a href="http://www.brunswick.com">Brunswick</a></td>
<td>Finance</td>
<td>Transportation</td>
<td>Transportation Equipment</td>
</tr>
<tr>
<td><a href="http://www.darlingii.com">Darling Ingredients</a></td>
<td>Environmental/Sustainability</td>
<td>Food, Beverages &amp; Tobacco</td>
<td>Food Production</td>
</tr>
<tr>
<td><a href="http://www.genesisenergy.com">Genesis Energy</a></td>
<td>Investment Financial Services</td>
<td>Energy</td>
<td>Pipelines</td>
</tr>
<tr>
<td><a href="http://www.watsco.com">Watsco</a></td>
<td>Investment/Finance</td>
<td>Wholesalers</td>
<td>Wholesalers: Diversified</td>
</tr>
<tr>
<td><a href="http://www.xyleminc.com">Xylem</a></td>
<td>Environment</td>
<td>Industrials</td>
<td>Industrial Machinery</td>
</tr>
<tr>
<td><a href="http://www.brinks.com">Brinks</a></td>
<td>Finance</td>
<td>Business Services</td>
<td>Diversified Outsourcing Services</td>
</tr>
<tr>
<td><a href="http://www.bigheartpet.com">Big Heart Pet</a></td>
<td>Animal Welfare</td>
<td>Food, Beverages &amp; Tobacco</td>
<td>Food Consumer Products</td>
</tr>
<tr>
<td><a href="http://www.unisys.com">Unisys</a></td>
<td>Logistics/Business</td>
<td>Technology</td>
<td>Information Technology Services</td>
</tr>
<tr>
<td><a href="http://www.hyster-yale.com">Hyster-Yale</a></td>
<td>Business/Finance Corporation</td>
<td>Industrials</td>
<td>Industrial Machinery</td>
</tr>
<tr>
<td><a href="http://www.schnitzersteel.com">Schnitzer Steel</a></td>
<td>Corporate/Business</td>
<td>Materials</td>
<td>Metals</td>
</tr>
<tr>
<td><a href="http://www.aointl.com">AOI</a></td>
<td>Business/Company Information</td>
<td>Food, Beverages &amp; Tobacco</td>
<td>Tobacco</td>
</tr>
<tr>
<td><a href="http://www.newmarket.com">NewMarket</a></td>
<td>Company/Corporate Finance</td>
<td>Chemicals</td>
<td>Chemicals</td>
</tr>
<tr>
<td><a href="http://www.gbrx.com">Greenbrier</a></td>
<td>Company/Employment</td>
<td>Transportation</td>
<td>Transportation Equipment</td>
</tr>
<tr>
<td><a href="http://www.universalamerican.com">Universal American</a></td>
<td>Gaming, Entertainment</td>
<td>Health Care</td>
<td>Health Care: Insurance and Managed Care</td>
</tr>
</tbody>
</table>
<p> <strong>*Note: All the financial/crypto/insurance sites were categorised with 100 percent accuracy.</strong></p>
<p> <strong>Then we scraped the same 1000 websites that we did with Facebook Zero Shot Model.</strong></p>
<p> Again the model gave more than 90 percent accuracy. But the major problem was coming in categorizing pornographic websites.</p>
<hr>
<p><strong>To make this project completely foolproof, we will apply 3 layers of predictions. First we will categorize a large number of websites using above mentioned models. Then we will make our own model that will be trained on that labelled dataset. Finally with these 3 layers of categorization we will be able to make the whole project extremely accurate.</strong></p>
<hr>
<h3 id="-detailed-hardware-requirements-cpu-offloading-table-"><strong>Detailed Hardware Requirements &amp; CPU Offloading Table</strong></h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>Size</th>
<th>VRAM Requirement (GPU Memory)</th>
<th>RAM Requirement (CPU)</th>
<th>CPU Offloading Potential</th>
<th>Performance Impact</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>405b-instruct-fp16</strong></td>
<td><strong>812GB</strong></td>
<td><strong>&gt;800GB</strong> (H100/A100 cluster)</td>
<td>1.5-2TB</td>
<td>Not recommended</td>
<td>Severe slowdown, not feasible for CPUs</td>
</tr>
<tr>
<td><strong>405b-instruct-q2_K</strong></td>
<td>149GB</td>
<td><strong>~160GB</strong> (8x A100 80GB)</td>
<td>256-320GB</td>
<td>Possible (50-70%)</td>
<td>Heavy latency (5-10x slower)</td>
</tr>
<tr>
<td><strong>405b-instruct-q3_K_L</strong></td>
<td>213GB</td>
<td><strong>~220GB</strong> (4-8x A100 80GB)</td>
<td>320-512GB</td>
<td>Partial (40-60%)</td>
<td>Noticeable latency increase (~4-6x)</td>
</tr>
<tr>
<td><strong>405b-instruct-q3_K_M</strong></td>
<td>195GB</td>
<td><strong>~200GB</strong> (4x A100 80GB)</td>
<td>320GB</td>
<td>Partial (40-60%)</td>
<td>Medium latency impact (~3-5x)</td>
</tr>
<tr>
<td><strong>405b-instruct-q3_K_S</strong></td>
<td>175GB</td>
<td><strong>~180GB</strong> (4x A100 80GB)</td>
<td>256GB</td>
<td>Partial (30-50%)</td>
<td>Moderate slowdown (~3-4x)</td>
</tr>
<tr>
<td><strong>405b-instruct-q4_0</strong></td>
<td>229GB</td>
<td><strong>~240GB</strong> (4-8x A100 80GB)</td>
<td>384GB</td>
<td>Moderate (30-40%)</td>
<td>Increased inference time (~2-3x)</td>
</tr>
<tr>
<td><strong>405b-instruct-q4_1</strong></td>
<td>254GB</td>
<td><strong>~260GB</strong> (4-8x A100 80GB)</td>
<td>512GB</td>
<td>Moderate (30-40%)</td>
<td>Slight slowdown (~2-3x)</td>
</tr>
<tr>
<td><strong>405b-instruct-q4_K_M</strong></td>
<td>243GB</td>
<td><strong>~250GB</strong> (4-8x A100 80GB)</td>
<td>512GB</td>
<td>Partial (40-50%)</td>
<td>Acceptable latency (~1.5-2.5x)</td>
</tr>
<tr>
<td><strong>405b-instruct-q4_K_S</strong></td>
<td>231GB</td>
<td><strong>~240GB</strong> (4-8x A100 80GB)</td>
<td>384GB</td>
<td>Moderate (30-40%)</td>
<td>Noticeable but usable (~2-3x)</td>
</tr>
<tr>
<td><strong>405b-instruct-q5_0</strong></td>
<td>279GB</td>
<td><strong>~280GB</strong> (4-8x A100 80GB)</td>
<td>512GB+</td>
<td>Limited (20-30%)</td>
<td>Mild slowdown (~1.5-2x)</td>
</tr>
<tr>
<td><strong>405b-instruct-q5_1</strong></td>
<td>305GB</td>
<td><strong>~320GB</strong> (4-8x A100 80GB)</td>
<td>512GB+</td>
<td>Limited (20-30%)</td>
<td>Minor impact (~1.2-1.8x)</td>
</tr>
<tr>
<td><strong>405b-instruct-q5_K_M</strong></td>
<td>287GB</td>
<td><strong>~290GB</strong> (4-8x A100 80GB)</td>
<td>512GB+</td>
<td>Very limited (10-20%)</td>
<td>Slight latency (~1.1-1.5x)</td>
</tr>
<tr>
<td><strong>405b-instruct-q5_K_S</strong></td>
<td>279GB</td>
<td><strong>~280GB</strong> (4-8x A100 80GB)</td>
<td>512GB</td>
<td>Limited (10-20%)</td>
<td>Small impact (~1.1-1.5x)</td>
</tr>
<tr>
<td><strong>405b-instruct-q6_K</strong></td>
<td>333GB</td>
<td><strong>~340GB</strong> (8x A100 80GB)</td>
<td>768GB+</td>
<td>Not recommended</td>
<td>Too slow for practical use</td>
</tr>
<tr>
<td><strong>405b-instruct-q8_0</strong></td>
<td>431GB</td>
<td><strong>~440GB</strong> (8-16x A100 80GB)</td>
<td>1TB+</td>
<td>Not feasible</td>
<td>CPU offloading impractical</td>
</tr>
</tbody>
</table>
<hr>
</body>
</html>
