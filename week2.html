<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Finding ASR Models</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
</head>

<body class="bg-light center-text">
    <!-- Navbar -->
    <nav class="navbar navbar-expand-lg">
        <div class="container mb-auto">
            <a class="navbar-brand" href="#"><h2 class="text-center m-2">Week 2</h2></a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"
                aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link active" aria-current="page" href="index.html">Home</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="tests.html">Tested Examples</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#">Contact</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>
    <br>

    <!-- Hero Section -->
    <section class="bg-light text-dark text-align-center text-sm-start">
        <div class="container">
            <p class="lead my-4">
                In second week, our task was to find custom models that will take audio input and give Punjabi transcript as output. For this I started reading research papers about Punjabi ASR. Here are the list of papers that I read. <br> 
                <ul>
                    <li><a href="https://www.sciencedirect.com/science/article/pii/S2352340923009265">Real and synthetic Punjabi speech datasets for automatic speech recognition</a></li>
                    <li><a href="https://www.researchgate.net/publication/283816423_Punjabi_Speech_Recognition_A_Survey">Punjabi Speech Recognition: A Survey</a></li>
                    <li><a href="https://www.researchgate.net/publication/355891351_Building_an_ASR_System_for_Indian_Punjabi_language_and_its_evaluation_for_Malwa_and_Majha_dialect_Preliminary_Results">Building an ASR System for Indian (Punjabi) language and its evaluation for Malwa and Majha dialect: Preliminary Results</a></li>
                </ul>
            </p>         
            <p class="lead my-4">
                I read documentation of <a href="https://huggingface.co/docs/transformers/en/model_doc/llama3">Llama 3.1</a>, <a href="https://huggingface.co/docs/transformers/en/model_doc/wav2vec2">Wav2Vec 2.0</a>, <a href="https://ai4bharat.iitm.ac.in/indicwav2vec/">IndicWav2Vec</a>. These all are large language models. We can use these models in our app. These models can be downloaded from <a href="https://huggingface.co/">Huggingface Website.</a> Llama 3.1 is the largest and the most accurate LLM model in the world. It is developed by Meta. It is trained on 405 Billion parameters. IndicWav2Vec is specifially trained on Indian Languages like Bhashini AI. IndicWav2Vec supports only 9 Indian languages. But none of these models supported Punjabi language. So it all came to a dead end. 
            </p>      
            <p class="lead my-4">
                However I did found a dataset which had almost 40,000 examples with audio input, punjabi transcript and english translation. This dataset is of 10.9 GBs. Here is the link <a href="https://huggingface.co/datasets/aaparajit02/punjabi-asr">to the dataset. The special thing about this dataset is that it actually takes aud samples from old punjabi newscasters. These are real audio clips by real people.</a><br>
                In another dataset I got more than 1,25,000 examples with audio input and Punjabi transcript. It is of 60 GBs. This is the <a href="https://huggingface.co/datasets/kdcyberdude/Punjabi_ASR_datasets">link to that dataset</a>. This dataset consists of audio clips by AI voice. It has been collected by using text-to-speech. The downside of this is that the model can be overfitted on this dataset.
            </p>   
            <p class="lead my-4">
                I also tried to find custom models available on github. I did find one. <a href="https://ohmvikrant.github.io/Punjabi-ASR/">This</a> is the link to one of the articles. However it was not relevant for our project.
            </p>   
            <p class="lead my-4">
                We also tried to implement OpenAI Whisper and Google Transcript API but at the time we did not have their api key as they are paid. They will be further explained in <a href="week3.html">Week 3.</a>
            </p>
            <!-- Navigation buttons -->
            <div class="flex justify-center space-x-4 text-center mb-4 bg-blue-500 hover:bg-blue-700 text-black font-bold py-2 px-4 rounded">
                <a href="week1.html">
                    <button>
                        1
                    </button>
                </a>
                <a href="week3.html">
                    <button>
                        3
                    </button>
                </a>
                <a href="week4.html">
                    <button>
                        4
                    </button>
                </a>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="bg-dark text-white text-center p-4">
        <p class="m-0">Â© 2024 Transcript API's Documentations.</p>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>

</html>
